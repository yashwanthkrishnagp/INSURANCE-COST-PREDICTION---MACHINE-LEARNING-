# -*- coding: utf-8 -*-
"""INSURANCE PREDICTOR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FkNn0VtfOsP5AsSCpsZmY9mdTqFG-5RQ

# PROJECT : HEALTH INSURANCE PREDICTION
   - DATE : 19.03.2024
   
   
   - NAME :
   
- DOMAIN : MACHINE LEARNIG

# DATA SET
"""

#importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as py
import seaborn as sb

#reading the dataset (DS)

med_ins=pd.read_csv("insurance.csv")
print(med_ins)

print(med_ins.nunique())

print(med_ins["region"].unique())

#information of DS

print(med_ins.info())

#checking null values in DS

print(med_ins.isna().sum())

#displaying the columns

print(med_ins.columns)

py.figure(figsize=(5,8))
med_ins['age'].value_counts().plot(kind='pie',colormap='RdPu')
py.legend(title='Ages',bbox_to_anchor=(1.05,0.75),ncol=10,loc='best',frameon=False,prop ={'weight':'bold'},fontsize=6)

a=med_ins['children'].value_counts()
a.plot(kind='bar',color='pink',edgecolor='black')
py.legend(loc='best',frameon=False)
py.bar(range(len(a)),a,hatch='*',color='pink',edgecolor='white')

b=med_ins['bmi'].value_counts().plot(kind='hist',color='pink',edgecolor='black')
ax1=py.gca()
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
py.legend(loc='best',frameon=False,bbox_to_anchor=(0.9,0.99))

sb.catplot(x="sex",y="charges",kind='box',color='hotpink',data=med_ins)
#ax=py.axes()
#ax.set_facecolor("black")
ax1=py.gca()
ax1.spines['top'].set_visible(True)
ax1.spines['right'].set_visible(True)

sb.violinplot(x="sex",y="bmi",hue="smoker",data=med_ins)
py.legend(bbox_to_anchor=(1.17,0.17),frameon=True)

sb.catplot(x="region",y="charges",data=med_ins)

#decription of DS

print(med_ins.describe())

from sklearn.preprocessing import LabelEncoder
l=LabelEncoder()
med_ins.iloc[:,1]=l.fit_transform(med_ins.iloc[:,1])

med_ins.iloc[:,4]=l.fit_transform(med_ins.iloc[:,4])

med_ins.iloc[:,5]=l.fit_transform(med_ins.iloc[:,5])

print(med_ins)

"""## LinearRegression"""

#assign x and y

x=med_ins[['smoker']].values
y=med_ins[['charges']].values

#spliting

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)

#algorithm

from sklearn.linear_model import LinearRegression
lr=LinearRegression()
lr1=lr.fit(x_train,y_train)
print('COEFFICIENT:',lr1.coef_)
print('  INTERCEPT:',lr1.intercept_)

#prediction

y_pred=lr1.predict(x_test)
ax=py.axes()
ax1=py.gca()
ax.set_facecolor("white")
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
py.scatter(x_test,y_test,s=30,color="pink")
py.plot(x_test,y_pred,color="black",linewidth=4,linestyle=(0,(0.1,2)),dash_capstyle="round")

#Difference prediction

res=y_test-y_pred
y=[i for i in range(1,len(res)+1)]
ax=py.axes()
ax1=py.gca()
ax.set_facecolor("white")
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
py.scatter(y,res,s=30,color='pink')
py.plot(y,[0]*len(y_pred),color='black',linewidth=4,linestyle=(0,(0.1,2)),dash_capstyle="round")

from sklearn.metrics import r2_score,mean_squared_error
mse = mean_squared_error(y_test,y_pred)
print("ROOT MEAN SQUARED ERROR : ",np.sqrt(mse))
print("     MEAN SQUARED ERROR : ",mse)
A1_=r2_score(y_test,y_pred)
print("         ACCUARCY SCORE :",A1_)

"""## PRE PROCESSING"""

#assign x and y

x=med_ins.iloc[:,0:-1].values
y=med_ins.iloc[:,-1].values

#split

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)

#Normalize

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.fit_transform(x_test)

"""## DECISION TREE REGRESSOR"""

#Algorithm

from sklearn.tree import DecisionTreeRegressor
dtr  = DecisionTreeRegressor()
dtr1 = dtr.fit(x_train,y_train)

#Predict
y_pred = dtr1.predict(x_test)
from sklearn.metrics import r2_score,mean_squared_error
a1=mean_squared_error(y_test,y_pred)
r1=print('ROOT MEAN SQUARED ERROR : ',np.sqrt(a1))
print("     MEAN SQUARED ERROR : ",a1)
B1_=r2_score(y_test,y_pred)
print('         ACCUARCY SCORE :',B1_)

"""## NAIVE BAYES"""

#Algorithm

from sklearn.linear_model import BayesianRidge
BR=BayesianRidge()
BR2=BR.fit(x_train,y_train)

#predict

y_pred=BR2.predict(x_test)
from sklearn.metrics import r2_score,mean_squared_error
a1=mean_squared_error(y_test,y_pred)
r1=print('ROOT MEAN SQUARED ERROR : ',np.sqrt(a1))
print("     MEAN SQUARED ERROR : ",a1)
C1_=r2_score(y_test,y_pred)
print('         ACCUARCY SCORE :',C1_)

"""## RANDOM FOREST TREE"""

#ALGORITHM

from sklearn.ensemble import RandomForestRegressor
rf=RandomForestRegressor()
rf1=rf.fit(x_train,y_train)

#predict

y_pred=rf1.predict(x_test)
from sklearn.metrics import r2_score,mean_squared_error
a1=mean_squared_error(y_test,y_pred)
r1=print('ROOT MEAN SQUARED ERROR : ',np.sqrt(a1))
print("     MEAN SQUARED ERROR : ",a1)
D1_=r2_score(y_test,y_pred)
print('         ACCUARCY SCORE :',D1_)

"""##  K-NEAREST NEIGHBOUR"""

#ALGORITHM

from sklearn.neighbors import KNeighborsRegressor
kn=KNeighborsRegressor(n_neighbors=5,metric='minkowski',p=2)
kn1=kn.fit(x_train,y_train)

#predict

y_pred=kn1.predict(x_test)
from sklearn.metrics import r2_score,mean_squared_error
a1=mean_squared_error(y_test,y_pred)
r1=print('ROOT MEAN SQUARED ERROR : ',np.sqrt(a1))
print("     MEAN SQUARED ERROR : ",a1)
E1_=r2_score(y_test,y_pred)
print('         ACCUARCY SCORE :',E1_)

"""## SUPPORT VECTOR MACHINE (SVM)"""

#ALGORITHM

from sklearn.svm import SVR
sv=SVR()
sv1=sv.fit(x_train,y_train)

#predict
y_pred=sv1.predict(x_test)

from sklearn.metrics import r2_score,mean_squared_error
a1=mean_squared_error(y_test,y_pred)
r1=print('ROOT MEAN SQUARED ERROR : ',np.sqrt(a1))
print("     MEAN SQUARED ERROR : ",a1)
F1_=r2_score(y_test,y_pred)
print('         ACCUARCY SCORE :',F1_)

"""## FEATURE IMPORTANCE"""

lst=[A1_,B1_,C1_,D1_,E1_,F1_]
print(       )
print('                                   ALGORITHM ACCURACY SCORES       ')
print(     )
print(     )
print("LINEAR REGRESSION ACCUARCY SCORE : ",A1_)
print("RANDOM FOREST ACCURACY SCORE     : ",D1_)
print("DECISION TREE ACCURACY SCORE     : ",B1_)
print("NAIVE BAYES ACCURACY SCORE       : ",C1_)
print("KNN ACCURACY SCORE               : ",E1_)
print("SVM ACCURACY SCORE               :",F1_)
print(       )
print(       )
print(       )

max_=max(lst)

if max_==A1_:

    print('SELECTED ALGORITHM : LINEAR ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCURACY SCORE    :',max_)
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance=lr.feature_importances_
    for i,v in enumerate(importance):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index=med_ins.columns[:-1]
    importance=pd.Series(rf.feature_importances_,index=index)
    importance.nlargest(13).plot(kind='barh',color='pink')

elif max_==B1_:

    print('SELECTED ALGORITHM : DECISION TREE: ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCURACY SCORE    :',max_)
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance1=dtr.feature_importances_
    for i,v in enumerate(importance1):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index1=med_ins.columns[:-1]
    importance1=pd.Series(rf.feature_importances_,index=index1)
    importance1.nlargest(13).plot(kind='barh',color='pink')

elif max_==C1_:
    print('SELECTED ALGORITHM : NAIVE BAYES: ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCURACY SCORE    :',max_)
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance2=BR.feature_importances_
    for i,v in enumerate(importance2):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index2=med_ins.columns[:-1]
    importance2=pd.Series(rf.feature_importances_,index=index2)
    importance2.nlargest(13).plot(kind='barh',color='pink')


elif max_==D1_:

    print('SELECTED ALGORITHM : RANDOM FOREST ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCUARACY SCORE    :',max_)
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance3=rf.feature_importances_
    for i,v in enumerate(importance3):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index3=med_ins.columns[:-1]
    importance3=pd.Series(rf.feature_importances_,index=index3)
    importance3.nlargest(13).plot(kind='barh',color='pink')

elif max_==E1_:

    print('SELECTED ALGORITHM :KNN ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCUARACY SCORE    :',max_)
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance4=kn.feature_importances_
    for i,v in enumerate(importance4):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index4=med_ins.columns[:-1]
    importance4=pd.Series(rf.feature_importances_,index=index4)
    importance4.nlargest(13).plot(kind='barh',color='pink')

elif max_==F1_:
    print('SVM:',max_)
    print('SELECTED ALGORITHM : SVM ')
    print('TYPE OF ALGORITHM  : REGRESSION')
    print('ACCUARACY SCORE    :',max_)
    print(       )
    print(       )
    print('CONTRIBUTION')
    print(       )

    importance5=sv.feature_importances_
    for i,v in enumerate(importance5):
        print("Feature: %0d ; Score: %5f"%(i,v))

    index5=med_ins.columns[:-1]
    importance5=pd.Series(rf.feature_importances_,index=index5)
    importance5.nlargest(13).plot(kind='barh',color='pink')

else:
    print('OOPS!! SOMETHING WENT WRONG')

"""## PREDICTIONS"""

print("Amount that can be claimed for health insurance:",rf.predict(sc.transform([[19,1,29.987,2,0,3]])))